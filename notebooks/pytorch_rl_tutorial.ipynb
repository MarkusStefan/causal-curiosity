{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d90103",
   "metadata": {},
   "source": [
    "# A PyTorch Tutorial for Reinforcement Learning\n",
    "\n",
    "Welcome to this interactive tutorial on using PyTorch to build and train Reinforcement Learning (RL) agents. The goal of this notebook is to guide you through the fundamentals of building RL agents that can learn to solve tasks in simulated environments from the [Gym](https://gymnasium.farama.org/) library.\n",
    "\n",
    "We will cover:\n",
    "1.  Interacting with Gym environments.\n",
    "2.  Building policy networks using `torch.nn`.\n",
    "3.  Implementing a complete policy gradient agent (REINFORCE).\n",
    "4.  Training the agent using PyTorch's automatic differentiation.\n",
    "5.  Adapting the agent for continuous control tasks in MuJoCo.\n",
    "\n",
    "This tutorial is designed to be hands-on. You will find several exercises where you'll need to fill in the missing code. This will help you solidify your understanding of the concepts.\n",
    "\n",
    "Let's get started! We'll assume you are working on a machine without a dedicated GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffee8b",
   "metadata": {},
   "source": [
    "## Part 1: Setting up the Environment and Interacting with Gym\n",
    "\n",
    "Before we can build an RL agent, we need to understand the environment it will interact with. We'll be using [Gymnasium](https://gymnasium.farama.org/) (a fork of OpenAI's Gym), which provides a wide variety of simulated environments for RL research. We will also use `torch` for building our neural networks and `matplotlib` for plotting our results. If you are using a MuJoCo environment, you will also need to have `mujoco` installed.\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3787ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1258bbb10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10639de",
   "metadata": {},
   "source": [
    "A Gym environment has a few key methods and attributes:\n",
    "\n",
    "*   `observation_space`: This defines the structure and range of the states you can observe.\n",
    "*   `action_space`: This defines the set of possible actions the agent can take.\n",
    "*   `reset()`: This resets the environment to a starting state and returns that initial observation.\n",
    "*   `step(action)`: This executes an action in the environment and returns a tuple of `(observation, reward, terminated, truncated, info)`.\n",
    "    *   `observation`: The new state of the environment.\n",
    "    *   `reward`: The reward received for the last action.\n",
    "    *   `terminated`: A boolean indicating if the episode has ended (e.g., the pole fell over).\n",
    "    *   `truncated`: A boolean indicating if the episode was cut short (e.g., reached a time limit).\n",
    "    *   `info`: A dictionary with auxiliary diagnostic information.\n",
    "\n",
    "Let's see this in action with the `CartPole-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9413994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space size: 4\n",
      "Action space size: 2\n",
      "Initial state: [ 0.04181642  0.00337763  0.03376347 -0.01761198]\n",
      "Action taken: 0\n",
      "Next state: [ 0.04188398 -0.19221185  0.03341123  0.2855296 ]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Get the state and action space sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(f\"State space size: {state_size}\")\n",
    "print(f\"Action space size: {action_size}\")\n",
    "\n",
    "# Reset the environment\n",
    "state, info = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# Take a random action\n",
    "action = env.action_space.sample()\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"Action taken: {action}\")\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Truncated: {truncated}\")\n",
    "print(f\"Info: {info}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e653f",
   "metadata": {},
   "source": [
    "## Part 2: Building a Neural Network with PyTorch\n",
    "\n",
    "Our RL agent will use a neural network to decide which action to take based on the current state. This network is called a **policy network**. For a given state, it will output a probability distribution over the possible actions.\n",
    "\n",
    "We can easily create neural networks in PyTorch by creating a class that inherits from `torch.nn.Module`. We define the layers of our network in the `__init__` method and specify how data flows through them in the `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e671ce",
   "metadata": {},
   "source": [
    "### Exercise: Create a Policy Network\n",
    "\n",
    "Now it's your turn! Your task is to create a simple policy network for the `CartPole-v1` environment. The network should take the state as input and output the probabilities for the two possible actions (move left or right).\n",
    "\n",
    "The network should have:\n",
    "*   An input layer that accepts the state (size `state_size`).\n",
    "*   One hidden layer with 128 neurons and a ReLU activation function.\n",
    "*   An output layer that produces the action probabilities (size `action_size`). We'll use a softmax activation on the output to ensure the probabilities sum to 1.\n",
    "\n",
    "Fill in the missing code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(\n",
      "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected neural network policy for reinforcement learning.\n",
    "    Maps:\n",
    "        State --> hidden dim --> Action\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(Policy, self).__init__()\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)# ... Fill in the missing code: a linear layer from state_size to hidden_size\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)# ... Fill in the missing code: a linear layer from hidden_size to action_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ... Fill in the missing code: apply a ReLU activation to the first layer's output\n",
    "        x = F.relu(self.fc1(x))# ...\n",
    "        # ... Fill in the missing code: get the output from the second layer\n",
    "        x = F.relu(self.fc2(x))# ...\n",
    "        # Apply a softmax to get action probabilities\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "# Example of how to create the policy network\n",
    "policy = Policy(state_size, action_size)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989aa230",
   "metadata": {},
   "source": [
    "## Part 3: Training a Simple Agent with Policy Gradients (REINFORCE)\n",
    "\n",
    "Now that we have a policy network, we need a way to train it. We'll use a policy gradient algorithm called **REINFORCE**. The main idea behind REINFORCE is to increase the probability of actions that lead to high rewards and decrease the probability of actions that lead to low rewards.\n",
    "\n",
    "To do this, we will:\n",
    "1.  Run an episode using the current policy and collect the trajectory of states, actions, and rewards.\n",
    "2.  For each step in the trajectory, calculate the **discounted future return**.\n",
    "3.  Calculate the **policy loss**.\n",
    "4.  Update the policy network's weights using backpropagation.\n",
    "\n",
    "### Action Selection\n",
    "\n",
    "Given the action probabilities from our policy network, we need a way to sample an action. We can use `torch.distributions.Categorical` to create a distribution object from which we can sample an action and also get the log probability of that action, which we'll need for the loss calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cda7f",
   "metadata": {},
   "source": [
    "### Exercise: Implement the REINFORCE Training Loop\n",
    "\n",
    "Your next task is to implement the main training loop for the REINFORCE agent. We'll break this down into a few functions.\n",
    "\n",
    "First, a function to collect a trajectory from the environment using the policy. Then, the main training loop that uses this trajectory to update the policy.\n",
    "\n",
    "You will need to:\n",
    "1.  In the `collect_trajectory` function, loop until the episode is done. In each step, get the action probabilities, sample an action, get its log probability, and store the log probability and the reward.\n",
    "2.  In the main loop, after collecting a trajectory, calculate the discounted returns.\n",
    "3.  Calculate the policy loss. The loss for each step is `-log_prob * return`. The total loss is the sum of these values.\n",
    "4.  Use the optimizer to perform a gradient update.\n",
    "\n",
    "Fill in the missing code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333a49f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 2., 3., 4.]]) torch.Size([1, 4])\n",
      "z: tensor([[[1., 2., 3., 4.]]]) torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0]])  # Example input\n",
    "z = x.unsqueeze(0)  # Add batch dimension\n",
    "print(\"x:\", x, x.shape)\n",
    "print(\"z:\", z, z.shape)\n",
    "z[0, 0, 0].item() # only works if scalar is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83db238",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m optimizer = optim.Adam(policy.parameters(), lr=\u001b[32m1e-2\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m scores = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Plot the scores\u001b[39;00m\n\u001b[32m     73\u001b[39m plt.plot(np.arange(\u001b[38;5;28mlen\u001b[39m(scores)), scores)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(policy, env, optimizer, n_episodes, gamma)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# ... Fill in the missing code: calculate the policy loss for each step\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m log_prob, R \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(log_probs, returns):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mpolicy_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# ... Fill in the missing code: sum the policy loss and perform a gradient update\u001b[39;00m\n\u001b[32m     49\u001b[39m optimizer.zero_grad()\n",
      "\u001b[31mTypeError\u001b[39m: list.append() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "def collect_trajectory(policy, env):\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        # ... Fill in the missing code: get action probabilities from the policy\n",
    "        action_probs = policy(state_tensor)\n",
    "        \n",
    "        # Create a categorical distribution and sample an action\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # ... Fill in the missing code: get the log probability of the sampled action\n",
    "        log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        # Take a step in the environment ... extract scalar value from tensor\n",
    "        state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "    return log_probs, rewards\n",
    "\n",
    "def train(policy, env, optimizer, n_episodes=1000, gamma=0.99):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        log_probs, rewards = collect_trajectory(policy, env)\n",
    "        \n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        # Normalize returns for better stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-6) # add a small neglicible const. to prevent 0div\n",
    "        \n",
    "        policy_loss = []\n",
    "        # ... Fill in the missing code: calculate the policy loss for each step\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "            \n",
    "        # ... Fill in the missing code: sum the policy loss and perform a gradient update\n",
    "        optimizer.zero_grad() # reset optimizer\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        # ...\n",
    "        policy_loss.backward()\n",
    "        # ...\n",
    "        optimizer.step()\n",
    "        \n",
    "        scores.append(sum(rewards))\n",
    "        scores_window.append(sum(rewards))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "        if np.mean(scores_window) >= 195.0:\n",
    "            print(f'Environment solved in {i_episode-100:d} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "# Create the policy and optimizer\n",
    "policy = Policy(state_size, action_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "# Train the agent\n",
    "scores = train(policy, env, optimizer)\n",
    "\n",
    "# Plot the scores\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0d0df",
   "metadata": {},
   "source": [
    "## Part 4: Continuous Control with MuJoCo\n",
    "\n",
    "The `CartPole-v1` environment has a discrete action space (left or right). Many interesting problems, especially in robotics, have **continuous action spaces**, where actions are real-valued numbers (e.g., the amount of torque to apply to a motor).\n",
    "\n",
    "We'll use the `InvertedPendulum-v4` environment from `mujoco-py` to explore continuous control. The goal is to keep a pendulum upright.\n",
    "\n",
    "To handle continuous actions, we need to modify our policy network. Instead of outputting probabilities for discrete actions, the network will output the parameters of a probability distribution, typically a **Gaussian (Normal) distribution**. The network will output the `mean` and `standard deviation` of the distribution, and we will sample an action from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27388217",
   "metadata": {},
   "source": [
    "### Exercise: Create a Policy Network for Continuous Actions\n",
    "\n",
    "Your task is to create a policy network for the `InvertedPendulum-v4` environment.\n",
    "\n",
    "The network should:\n",
    "*   Take the state as input.\n",
    "*   Have one hidden layer.\n",
    "*   Have two output heads:\n",
    "    *   One for the `mean` of the action distribution.\n",
    "    *   One for the `standard deviation` (`log_std`) of the action distribution. We output the log of the standard deviation for numerical stability and ensure it's always positive by exponentiating it later.\n",
    "\n",
    "Fill in the missing code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa649af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create the environment and get its properties\n",
    "try:\n",
    "    env_continuous = gym.make('InvertedPendulum-v4')\n",
    "    continuous_state_size = env_continuous.observation_space.shape[0]\n",
    "    continuous_action_size = env_continuous.action_space.shape[0]\n",
    "    \n",
    "    class ContinuousPolicy(nn.Module):\n",
    "        def __init__(self, state_size, action_size, hidden_size=128):\n",
    "            super(ContinuousPolicy, self).__init__()\n",
    "            self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "            # ... Fill in the missing code: a linear layer for the mean\n",
    "            self.fc_mean = # ...\n",
    "            # ... Fill in the missing code: a linear layer for the log standard deviation\n",
    "            self.fc_log_std = # ...\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # ... Fill in the missing code: get the mean from the appropriate layer\n",
    "            mean = # ...\n",
    "            # ... Fill in the missing code: get the log_std from the appropriate layer\n",
    "            log_std = # ...\n",
    "            std = torch.exp(log_std) # ensure std is positive\n",
    "            return mean, std\n",
    "\n",
    "    # Example of how to create the policy network\n",
    "    continuous_policy = ContinuousPolicy(continuous_state_size, continuous_action_size)\n",
    "    print(continuous_policy)\n",
    "    env_continuous.close()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"MuJoCo not installed, skipping continuous control part.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with the MuJoCo environment: {e}\")\n",
    "    print(\"Skipping continuous control part.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad58886",
   "metadata": {},
   "source": [
    "The training process is very similar to the discrete case. The main difference is how we calculate the log probability of an action, which is required for the policy gradient loss. Instead of `Categorical`, we'll use `torch.distributions.Normal` to create our distribution, sample an action, and compute its log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591fcf37",
   "metadata": {},
   "source": [
    "### Exercise: Implement the Training Loop for Continuous Control\n",
    "\n",
    "Now, adapt the training loop for the continuous control case. The structure is almost identical to the discrete version.\n",
    "\n",
    "You will need to:\n",
    "1.  In the `collect_trajectory_continuous` function, get the `mean` and `std` from the policy.\n",
    "2.  Create a `Normal` distribution and sample an action.\n",
    "3.  Get the log probability of the sampled action.\n",
    "4.  The rest of the training loop (calculating returns and the policy update) remains the same.\n",
    "\n",
    "Fill in the missing code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory_continuous(policy, env):\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        # ... Fill in the missing code: get mean and std from the policy\n",
    "        mean, std = # ...\n",
    "        \n",
    "        # Create a normal distribution and sample an action\n",
    "        m = Normal(mean, std)\n",
    "        action = m.sample()\n",
    "        action = torch.clamp(action, min=env.action_space.low[0], max=env.action_space.high[0])\n",
    "        \n",
    "        # ... Fill in the missing code: get the log probability of the sampled action\n",
    "        log_probs.append(# ...)\n",
    "        \n",
    "        # Take a step in the environment\n",
    "        state, reward, terminated, truncated, _ = env.step(action.numpy().flatten())\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "    return log_probs, rewards\n",
    "\n",
    "def train_continuous(policy, env, optimizer, n_episodes=2000, gamma=0.99):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        log_probs, rewards = collect_trajectory_continuous(policy, env)\n",
    "        \n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-6)\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scores.append(sum(rewards))\n",
    "        scores_window.append(sum(rewards))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            \n",
    "    return scores\n",
    "\n",
    "try:\n",
    "    if 'env_continuous' in locals() and env_continuous is not None:\n",
    "        # Create the policy and optimizer\n",
    "        continuous_policy = ContinuousPolicy(continuous_state_size, continuous_action_size)\n",
    "        continuous_optimizer = optim.Adam(continuous_policy.parameters(), lr=1e-3)\n",
    "\n",
    "        # Train the agent\n",
    "        continuous_scores = train_continuous(continuous_policy, env_continuous, continuous_optimizer)\n",
    "\n",
    "        # Plot the scores\n",
    "        plt.plot(np.arange(len(continuous_scores)), continuous_scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "\n",
    "        env_continuous.close()\n",
    "except NameError:\n",
    "    print(\"Skipping continuous training because the environment was not created.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during continuous training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba58e8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully built and trained Reinforcement Learning agents for both discrete and continuous control tasks using PyTorch.\n",
    "\n",
    "In this tutorial, you have learned how to:\n",
    "*   Interact with Gym environments.\n",
    "*   Build neural networks with `torch.nn.Module`.\n",
    "*   Implement the REINFORCE algorithm from scratch.\n",
    "*   Use PyTorch's automatic differentiation to train your policy network.\n",
    "*   Adapt your agent for continuous action spaces.\n",
    "\n",
    "### Further Learning\n",
    "\n",
    "This tutorial covers the basics, but the field of Deep Reinforcement Learning is vast. Here are some topics you might want to explore next:\n",
    "\n",
    "*   **Actor-Critic Methods**: These methods, like A2C and A3C, combine policy gradients with a value function to learn more efficiently.\n",
    "*   **Proximal Policy Optimization (PPO)**: A state-of-the-art policy gradient method that is widely used and often more stable than REINFORCE.\n",
    "*   **Deep Q-Networks (DQN)**: A popular value-based method for discrete action spaces.\n",
    "*   **Soft Actor-Critic (SAC)**: An advanced off-policy actor-critic method for continuous control that incorporates entropy maximization for better exploration.\n",
    "\n",
    "I hope this tutorial has provided you with a solid foundation for your journey into Reinforcement Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAUSALVENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
